{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL connection string\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:test01!@localhost/amazon_india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22038011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Attempting to load transactions data from 'D:\\amazon_india_project\\data\\Cleaned-dataset\\cleaned_amazon_india_2015_2025.csv'...\n",
      "  Loaded chunk 1 into 'transactions' table...\n",
      "  Loaded chunk 2 into 'transactions' table...\n",
      "  Loaded chunk 3 into 'transactions' table...\n",
      "  Loaded chunk 4 into 'transactions' table...\n",
      "  Loaded chunk 5 into 'transactions' table...\n",
      "  Loaded chunk 6 into 'transactions' table...\n",
      "  Loaded chunk 7 into 'transactions' table...\n",
      "  Loaded chunk 8 into 'transactions' table...\n",
      "  Loaded chunk 9 into 'transactions' table...\n",
      "  Loaded chunk 10 into 'transactions' table...\n",
      "  Loaded chunk 11 into 'transactions' table...\n",
      "  Loaded chunk 12 into 'transactions' table...\n",
      "  Loaded chunk 13 into 'transactions' table...\n",
      "  Loaded chunk 14 into 'transactions' table...\n",
      "  Loaded chunk 15 into 'transactions' table...\n",
      "  Loaded chunk 16 into 'transactions' table...\n",
      "  Loaded chunk 17 into 'transactions' table...\n",
      "  Loaded chunk 18 into 'transactions' table...\n",
      "  Loaded chunk 19 into 'transactions' table...\n",
      "  Loaded chunk 20 into 'transactions' table...\n",
      "  Loaded chunk 21 into 'transactions' table...\n",
      "  Loaded chunk 22 into 'transactions' table...\n",
      "  Loaded chunk 23 into 'transactions' table...\n",
      "  Loaded chunk 24 into 'transactions' table...\n",
      "  Loaded chunk 25 into 'transactions' table...\n",
      "  Loaded chunk 26 into 'transactions' table...\n",
      "  Loaded chunk 27 into 'transactions' table...\n",
      "  Loaded chunk 28 into 'transactions' table...\n",
      "  Loaded chunk 29 into 'transactions' table...\n",
      "  Loaded chunk 30 into 'transactions' table...\n",
      "  Loaded chunk 31 into 'transactions' table...\n",
      "  Loaded chunk 32 into 'transactions' table...\n",
      "  Loaded chunk 33 into 'transactions' table...\n",
      "  Loaded chunk 34 into 'transactions' table...\n",
      "  Loaded chunk 35 into 'transactions' table...\n",
      "  Loaded chunk 36 into 'transactions' table...\n",
      "  Loaded chunk 37 into 'transactions' table...\n",
      "  Loaded chunk 38 into 'transactions' table...\n",
      "  Loaded chunk 39 into 'transactions' table...\n",
      "  Loaded chunk 40 into 'transactions' table...\n",
      "  Loaded chunk 41 into 'transactions' table...\n",
      "  Loaded chunk 42 into 'transactions' table...\n",
      "  Loaded chunk 43 into 'transactions' table...\n",
      "  Loaded chunk 44 into 'transactions' table...\n",
      "  Loaded chunk 45 into 'transactions' table...\n",
      "  Loaded chunk 46 into 'transactions' table...\n",
      "  Loaded chunk 47 into 'transactions' table...\n",
      "  Loaded chunk 48 into 'transactions' table...\n",
      "  Loaded chunk 49 into 'transactions' table...\n",
      "  Loaded chunk 50 into 'transactions' table...\n",
      "  Loaded chunk 51 into 'transactions' table...\n"
     ]
    }
   ],
   "source": [
    "# Add cleaned transaction details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError # Import for better error handling\n",
    "import os # Import os for path handling if needed, though not directly used here\n",
    "\n",
    "# ==========================\n",
    "# MySQL Connection Settings\n",
    "# ==========================\n",
    "USER = \"root\"              # Your MySQL username (or 'amazon_user' if you created one)\n",
    "PASSWORD = \"test01!\"       # Your MySQL password\n",
    "HOST = \"localhost\"         # Host\n",
    "PORT = 3306                # MySQL port\n",
    "DB = \"amazon_india\"        # Database name\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "# Added pool_pre_ping=True for better connection management\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# ==========================\n",
    "# Path to your cleaned transactions CSV\n",
    "# ==========================\n",
    "transactions_csv_path = r\"D:\\amazon_india_project\\data\\Cleaned-dataset\\cleaned_amazon_india_2015_2025.csv\"\n",
    "\n",
    "print(f\"üìÇ Attempting to load transactions data from '{transactions_csv_path}'...\")\n",
    "\n",
    "try:\n",
    "# Load cleaned transactions dataset\n",
    "# Explicitly parse 'order_date' during loading\n",
    "# We load the full DataFrame once for initial processing (like boolean conversion)\n",
    "# and then use pd.read_csv again in chunks for actual loading to SQL.\n",
    "# This avoids potential issues if the first read_csv is also chunked and needs bool_cols info.\n",
    "    initial_df = pd.read_csv(transactions_csv_path, parse_dates=['order_date'])\n",
    "\n",
    "    # Convert boolean columns to int (0 or 1) for MySQL's TINYINT(1)\n",
    "    bool_cols = ['is_prime_member', 'is_festival_sale', 'is_prime_eligible']\n",
    "    for col in bool_cols:\n",
    "        if col in initial_df.columns: # Check if column exists\n",
    "            initial_df[col] = initial_df[col].astype(int)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Boolean column '{col}' not found in initial DataFrame.\")\n",
    "\n",
    "    # --- IMPORTANT: Ensure your SQL table schema matches the DataFrame columns ---\n",
    "    # The `transactions` table should be created with VARCHAR for IDs and DATE for dates.\n",
    "    # If you're using the CREATE TABLE statement I provided previously, it should match.\n",
    "\n",
    "    # Load into MySQL using chunks\n",
    "    # Using chunksize for large datasets to prevent memory issues and improve stability\n",
    "    chunksize = 10000 # Adjust chunksize as needed based on your system's memory\n",
    "\n",
    "    # Re-read CSV in chunks for the actual SQL insertion\n",
    "    # This ensures that if initial_df is huge, we don't hold it all in memory\n",
    "    # while also allowing pre-processing like boolean conversion per chunk.\n",
    "    for i, chunk in enumerate(pd.read_csv(transactions_csv_path, chunksize=chunksize, parse_dates=['order_date'])):\n",
    "        # Apply boolean conversion to each chunk\n",
    "        for col in bool_cols:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = chunk[col].astype(int)\n",
    "        \n",
    "        # Handle potential NaN values in numeric columns that might be NOT NULL in SQL\n",
    "        # For example, if 'delivery_days' or 'customer_rating' are NOT NULL in SQL\n",
    "        # and have NaNs in DataFrame, this will cause an error.\n",
    "        # Fill NaNs with a default value (e.g., 0, -1) or the mean/median if necessary.\n",
    "        # Example: chunk['delivery_days'].fillna(0, inplace=True)\n",
    "        # Example: chunk['customer_rating'].fillna(0.0, inplace=True)\n",
    "        \n",
    "        chunk.to_sql('transactions', con=engine, if_exists='append', index=False)\n",
    "        print(f\"  Loaded chunk {i+1} into 'transactions' table...\")\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded all data into 'transactions' table.\")\n",
    "\n",
    "    # Verify row count\n",
    "    with engine.connect() as conn:\n",
    "        trans_count = conn.execute(\"SELECT COUNT(*) FROM transactions\").fetchone()[0]\n",
    "        print(f\"üìä Final row count in 'transactions' table: {trans_count}\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"‚ùå Database error during loading: {e}\")\n",
    "    # Explicitly rollback the transaction if an error occurs\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()\n",
    "    print(\"‚ö†Ô∏è Rolled back transaction.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "\n",
    "    print(\"\\nüöÄ Transactions data loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Attempting to load transactions data from 'D:\\amazon_india_project\\data\\Cleaned-dataset\\cleaned_amazon_india_2015_2025.csv'...\n",
      "üÜî Last loaded transaction_id in DB: TXN_2025_00076999\n",
      "‚è© Skipping chunk 1 (already loaded)\n",
      "‚è© Skipping chunk 2 (already loaded)\n",
      "‚è© Skipping chunk 3 (already loaded)\n",
      "‚è© Skipping chunk 4 (already loaded)\n",
      "‚è© Skipping chunk 5 (already loaded)\n",
      "‚è© Skipping chunk 6 (already loaded)\n",
      "‚è© Skipping chunk 7 (already loaded)\n",
      "‚è© Skipping chunk 8 (already loaded)\n",
      "‚è© Skipping chunk 9 (already loaded)\n",
      "‚è© Skipping chunk 10 (already loaded)\n",
      "‚è© Skipping chunk 11 (already loaded)\n",
      "‚è© Skipping chunk 12 (already loaded)\n",
      "‚è© Skipping chunk 13 (already loaded)\n",
      "‚è© Skipping chunk 14 (already loaded)\n",
      "‚è© Skipping chunk 15 (already loaded)\n",
      "‚è© Skipping chunk 16 (already loaded)\n",
      "‚è© Skipping chunk 17 (already loaded)\n",
      "‚è© Skipping chunk 18 (already loaded)\n",
      "‚ñ∂Ô∏è Resuming from chunk 19 after transaction_id TXN_2025_00076999\n",
      "‚úÖ Loaded chunk 19 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 20 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 21 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 22 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 23 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 24 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 25 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 26 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 27 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 28 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 29 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 30 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 31 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 32 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 33 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 34 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 35 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 36 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 37 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 38 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 39 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 40 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 41 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 42 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 43 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 44 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 45 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 46 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 47 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 48 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 49 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 50 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 51 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 52 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 53 into 'transactions' table...\n",
      "‚úÖ Loaded chunk 54 into 'transactions' table...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, MetaData, Table, text, inspect\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.dialects.mysql import insert as mysql_insert\n",
    "\n",
    "# ==========================\n",
    "# MySQL Connection Settings\n",
    "# ==========================\n",
    "USER = \"root\"\n",
    "PASSWORD = \"test01!\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "DB = \"amazon_india\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(\n",
    "    f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\",\n",
    "    pool_pre_ping=True\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Path to cleaned transactions CSV\n",
    "# ==========================\n",
    "transactions_csv_path = r\"D:\\amazon_india_project\\data\\Cleaned-dataset\\cleaned_amazon_india_2015_2025.csv\"\n",
    "\n",
    "print(f\"üìÇ Attempting to load transactions data from '{transactions_csv_path}'...\")\n",
    "\n",
    "# Define columns that should be integers, including boolean types\n",
    "int_cols = [\n",
    "    'is_prime_member', 'is_festival_sale', 'is_prime_eligible', \n",
    "    'quantity', 'delivery_days', 'order_month', 'order_year', 'order_quarter'\n",
    "]\n",
    "\n",
    "# Define columns that should be floats/decimals\n",
    "decimal_cols = [\n",
    "    'original_price_inr', 'discount_percent', 'discounted_price_inr',\n",
    "    'subtotal_inr', 'delivery_charges', 'final_amount_inr', \n",
    "    'product_weight_kg', 'customer_rating', 'product_rating'\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Reflect transactions table from DB to ensure metadata is up-to-date\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=engine)\n",
    "    transactions_table = metadata.tables['transactions']\n",
    "\n",
    "    # Detect last loaded transaction_id\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\n",
    "            text(\"SELECT transaction_id FROM transactions ORDER BY transaction_id DESC LIMIT 1\")\n",
    "        ).fetchone()\n",
    "        last_id_in_db = result[0] if result else None\n",
    "\n",
    "    if last_id_in_db:\n",
    "        print(f\"üÜî Last loaded transaction_id in DB: {last_id_in_db}\")\n",
    "    else:\n",
    "        print(\"üìÖ No existing data found in DB. Starting from the beginning.\")\n",
    "\n",
    "    chunksize = 10000\n",
    "    start_loading = False if last_id_in_db else True\n",
    "\n",
    "    # Read CSV in chunks\n",
    "    for i, chunk in enumerate(pd.read_csv(transactions_csv_path, chunksize=chunksize, parse_dates=['order_date'])):\n",
    "        \n",
    "        # Robust data type handling for integer columns\n",
    "        for col in int_cols:\n",
    "            if col in chunk.columns:\n",
    "                # Convert to numeric, coercing errors (e.g., text) to NaN.\n",
    "                numeric_col = pd.to_numeric(chunk[col], errors='coerce')\n",
    "                \n",
    "                # Round floating-point numbers to handle cases like 5.0 vs 5.5.\n",
    "                rounded_col = numeric_col.round()\n",
    "                \n",
    "                # Convert to nullable integer type, handling NaNs correctly.\n",
    "                chunk[col] = rounded_col.astype('Int64')\n",
    "\n",
    "        # Robust data type handling for decimal columns\n",
    "        for col in decimal_cols:\n",
    "            if col in chunk.columns:\n",
    "                # Convert to numeric, coercing errors to NaN.\n",
    "                numeric_col = pd.to_numeric(chunk[col], errors='coerce')\n",
    "                # Explicitly cast to float64, ensuring consistency.\n",
    "                chunk[col] = numeric_col.astype('float64')\n",
    "\n",
    "        # Replace ALL NaT/NaN values with None for MySQL compatibility\n",
    "        # This is a key step to catch any remaining NaNs, including in decimal columns\n",
    "        chunk = chunk.replace({np.nan: None})\n",
    "\n",
    "        # Resume logic\n",
    "        if last_id_in_db and not start_loading:\n",
    "            if last_id_in_db in chunk['transaction_id'].values:\n",
    "                idx = chunk.index[chunk['transaction_id'] == last_id_in_db][0]\n",
    "                chunk = chunk.loc[idx+1:]  # start after last loaded row\n",
    "                start_loading = True\n",
    "                if chunk.empty:\n",
    "                    print(f\"‚è© Skipping chunk {i+1} (all rows already loaded)\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"‚ñ∂Ô∏è Resuming from chunk {i+1} after transaction_id {last_id_in_db}\")\n",
    "            else:\n",
    "                print(f\"‚è© Skipping chunk {i+1} (already loaded)\")\n",
    "                continue\n",
    "        else:\n",
    "            start_loading = True\n",
    "\n",
    "        # Bulk UPSERT into MySQL\n",
    "        try:\n",
    "            with engine.begin() as conn:\n",
    "                rows = chunk.to_dict(orient='records')\n",
    "                if rows:\n",
    "                    stmt = mysql_insert(transactions_table).values(rows)\n",
    "                    stmt = stmt.on_duplicate_key_update(\n",
    "                        **{col.name: stmt.inserted[col.name] for col in transactions_table.columns}\n",
    "                    )\n",
    "                    conn.execute(stmt)\n",
    "            print(f\"‚úÖ Loaded chunk {i+1} into 'transactions' table...\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"‚ùå Error loading chunk {i+1}: {e}\")\n",
    "            break\n",
    "\n",
    "    # Final row count\n",
    "    with engine.connect() as conn:\n",
    "        trans_count = conn.execute(text(\"SELECT COUNT(*) FROM transactions\")).fetchone()[0]\n",
    "        print(f\"üìä Final row count in 'transactions' table: {trans_count}\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"‚ùå Database error during loading: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Transactions data loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c26048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Product catlog details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# ==========================\n",
    "# MySQL Connection Settings\n",
    "# ==========================\n",
    "USER = \"root\"              # Your MySQL username\n",
    "PASSWORD = \"test01!\"       # Your MySQL password\n",
    "HOST = \"localhost\"         # Host\n",
    "PORT = 3306                # MySQL port\n",
    "DB = \"amazon_india\"        # Database name\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# ==========================\n",
    "# Path to your product catalog CSV\n",
    "# ==========================\n",
    "products_csv_path = r\"D:\\amazon_india_project\\data\\Dataset\\amazon_india_products_catalog.csv\" # Adjust path as needed\n",
    "\n",
    "print(f\"üìÇ Attempting to load product catalog from '{products_csv_path}'...\")\n",
    "\n",
    "try:\n",
    "# Load product catalog dataset\n",
    "    products_df = pd.read_csv(products_csv_path)\n",
    "\n",
    "    # Convert boolean columns to int (0 or 1) for MySQL's TINYINT(1)\n",
    "    bool_cols = ['is_prime_eligible'] # Only this one seems to be boolean in your sample\n",
    "    for col in bool_cols:\n",
    "        if col in products_df.columns:\n",
    "            products_df[col] = products_df[col].astype(int)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Boolean column '{col}' not found in products DataFrame.\")\n",
    "\n",
    "    # --- IMPORTANT: Ensure your SQL table schema matches the DataFrame columns ---\n",
    "    # The `products` table should be created with VARCHAR for product_id.\n",
    "\n",
    "    # Load into MySQL\n",
    "    # Using chunksize for large datasets\n",
    "    chunksize = 500 # Products catalog is smaller, so chunksize can be adjusted\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(products_csv_path, chunksize=chunksize)):\n",
    "        # Apply boolean conversion to each chunk\n",
    "        for col in bool_cols:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = chunk[col].astype(int)\n",
    "        \n",
    "        chunk.to_sql('products', con=engine, if_exists='append', index=False)\n",
    "        print(f\"  Loaded chunk {i+1} into 'products' table...\")\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded all data into 'products' table.\")\n",
    "\n",
    "    # Verify row count\n",
    "    with engine.connect() as conn:\n",
    "        prod_count = conn.execute(\"SELECT COUNT(*) FROM products\").fetchone()[0]\n",
    "        print(f\"üìä Final row count in 'products' table: {prod_count}\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"‚ùå Database error during loading products: {e}\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()\n",
    "    print(\"‚ö†Ô∏è Rolled back transaction for products table.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred loading products: {e}\")\n",
    "\n",
    "    print(\"\\nüöÄ Product catalog loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d9c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 354969 unique customers into 'customers' table.\n"
     ]
    }
   ],
   "source": [
    "# Add unique customer details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "USER = \"root\"\n",
    "PASSWORD = \"test01!\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "DB = \"amazon_india\"\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# Load transactions data (or just read the cleaned CSV again)\n",
    "transactions_df = pd.read_csv(r\"D:\\amazon_india_project\\data\\cleaned_amazon_india_complete_2015_2025.csv\")\n",
    "\n",
    "# Extract unique customer data\n",
    "customers_df = transactions_df[[\n",
    "    'customer_id', 'customer_city', 'customer_state',\n",
    "    'customer_tier', 'customer_spending_tier', 'customer_age_group'\n",
    "]].drop_duplicates(subset=['customer_id'])\n",
    "\n",
    "try:\n",
    "    customers_df.to_sql('customers', con=engine, if_exists='append', index=False)\n",
    "    print(f\"‚úÖ Loaded {len(customers_df)} unique customers into 'customers' table.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading customers table: {e}\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec5c5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4015 new rows into time_dimension table.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "\n",
    "# Database connection setup\n",
    "USER = 'root'\n",
    "PASSWORD = 'test01!'\n",
    "HOST = 'localhost'\n",
    "PORT = 3306\n",
    "DB = 'amazon_india'\n",
    "engine = create_engine(f'mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}', pool_pre_ping=True)\n",
    "\n",
    "# Load and process data from the transactions CSV\n",
    "transactions_df = pd.read_csv(\n",
    "    r'D:\\amazon_india_project\\data\\cleaned_amazon_india_complete_2015_2025.csv', \n",
    "    parse_dates=['order_date']\n",
    ")\n",
    "\n",
    "# Filter for unique dates and drop any rows where 'date' is null (NaT)\n",
    "# This prevents the IntegrityError caused by trying to insert a NULL date.\n",
    "unique_dates = pd.DataFrame({'date': transactions_df['order_date'].unique()})\n",
    "unique_dates['date'] = pd.to_datetime(unique_dates['date'])\n",
    "unique_dates.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Create time dimension attributes, including only the columns that exist in your table\n",
    "time_df = pd.DataFrame({\n",
    "    'date': unique_dates['date'],\n",
    "    'year': unique_dates['date'].dt.year,\n",
    "    'month': unique_dates['date'].dt.month,\n",
    "    'quarter': unique_dates['date'].dt.quarter,\n",
    "})\n",
    "\n",
    "# Filter out dates that already exist in the database before inserting\n",
    "try:\n",
    "    existing_dates = pd.read_sql_table('time_dimension', con=engine, columns=['date'])\n",
    "    # Convert 'date' columns to the same data type for accurate comparison\n",
    "    new_dates_df = time_df[~time_df['date'].isin(existing_dates['date'])]\n",
    "\n",
    "    if not new_dates_df.empty:\n",
    "        new_dates_df.to_sql('time_dimension', con=engine, if_exists='append', index=False)\n",
    "        print(f'‚úÖ Loaded {len(new_dates_df)} new rows into time_dimension table.')\n",
    "    else:\n",
    "        print('‚úÖ No new unique dates to load.')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error loading time_dimension table: {e}')\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
