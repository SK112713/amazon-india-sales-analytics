{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL connection string\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:test01!@localhost/amazon_india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22038011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Attempting to load transactions data from 'D:\\amazon_india_project\\data\\Cleaned-dataset\\cleaned_amazon_india_2015_2025.csv'...\n",
      "  Loaded chunk 1 into 'transactions' table...\n",
      "  Loaded chunk 2 into 'transactions' table...\n",
      "  Loaded chunk 3 into 'transactions' table...\n",
      "  Loaded chunk 4 into 'transactions' table...\n",
      "  Loaded chunk 5 into 'transactions' table...\n",
      "  Loaded chunk 6 into 'transactions' table...\n",
      "  Loaded chunk 7 into 'transactions' table...\n",
      "  Loaded chunk 8 into 'transactions' table...\n",
      "  Loaded chunk 9 into 'transactions' table...\n",
      "  Loaded chunk 10 into 'transactions' table...\n",
      "  Loaded chunk 11 into 'transactions' table...\n",
      "  Loaded chunk 12 into 'transactions' table...\n",
      "  Loaded chunk 13 into 'transactions' table...\n",
      "  Loaded chunk 14 into 'transactions' table...\n",
      "  Loaded chunk 15 into 'transactions' table...\n",
      "  Loaded chunk 16 into 'transactions' table...\n",
      "  Loaded chunk 17 into 'transactions' table...\n",
      "  Loaded chunk 18 into 'transactions' table...\n",
      "  Loaded chunk 19 into 'transactions' table...\n",
      "  Loaded chunk 20 into 'transactions' table...\n",
      "  Loaded chunk 21 into 'transactions' table...\n",
      "  Loaded chunk 22 into 'transactions' table...\n",
      "  Loaded chunk 23 into 'transactions' table...\n",
      "  Loaded chunk 24 into 'transactions' table...\n",
      "  Loaded chunk 25 into 'transactions' table...\n",
      "  Loaded chunk 26 into 'transactions' table...\n",
      "  Loaded chunk 27 into 'transactions' table...\n",
      "  Loaded chunk 28 into 'transactions' table...\n",
      "  Loaded chunk 29 into 'transactions' table...\n",
      "  Loaded chunk 30 into 'transactions' table...\n",
      "  Loaded chunk 31 into 'transactions' table...\n",
      "  Loaded chunk 32 into 'transactions' table...\n",
      "  Loaded chunk 33 into 'transactions' table...\n",
      "  Loaded chunk 34 into 'transactions' table...\n",
      "  Loaded chunk 35 into 'transactions' table...\n",
      "  Loaded chunk 36 into 'transactions' table...\n",
      "  Loaded chunk 37 into 'transactions' table...\n",
      "  Loaded chunk 38 into 'transactions' table...\n",
      "  Loaded chunk 39 into 'transactions' table...\n",
      "  Loaded chunk 40 into 'transactions' table...\n",
      "  Loaded chunk 41 into 'transactions' table...\n",
      "  Loaded chunk 42 into 'transactions' table...\n",
      "  Loaded chunk 43 into 'transactions' table...\n",
      "  Loaded chunk 44 into 'transactions' table...\n",
      "  Loaded chunk 45 into 'transactions' table...\n",
      "  Loaded chunk 46 into 'transactions' table...\n",
      "  Loaded chunk 47 into 'transactions' table...\n",
      "  Loaded chunk 48 into 'transactions' table...\n",
      "  Loaded chunk 49 into 'transactions' table...\n",
      "  Loaded chunk 50 into 'transactions' table...\n",
      "  Loaded chunk 51 into 'transactions' table...\n"
     ]
    }
   ],
   "source": [
    "# Add cleaned transaction details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError # Import for better error handling\n",
    "import os # Import os for path handling if needed, though not directly used here\n",
    "\n",
    "# ==========================\n",
    "# MySQL Connection Settings\n",
    "# ==========================\n",
    "USER = \"root\"              # Your MySQL username (or 'amazon_user' if you created one)\n",
    "PASSWORD = \"test01!\"       # Your MySQL password\n",
    "HOST = \"localhost\"         # Host\n",
    "PORT = 3306                # MySQL port\n",
    "DB = \"amazon_india\"        # Database name\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "# Added pool_pre_ping=True for better connection management\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# ==========================\n",
    "# Path to your cleaned transactions CSV\n",
    "# ==========================\n",
    "transactions_csv_path = r\"D:\\amazon_india_project\\data\\Cleaned-dataset\\cleaned_amazon_india_2015_2025.csv\"\n",
    "\n",
    "print(f\"üìÇ Attempting to load transactions data from '{transactions_csv_path}'...\")\n",
    "\n",
    "try:\n",
    "# Load cleaned transactions dataset\n",
    "# Explicitly parse 'order_date' during loading\n",
    "# We load the full DataFrame once for initial processing (like boolean conversion)\n",
    "# and then use pd.read_csv again in chunks for actual loading to SQL.\n",
    "# This avoids potential issues if the first read_csv is also chunked and needs bool_cols info.\n",
    "    initial_df = pd.read_csv(transactions_csv_path, parse_dates=['order_date'])\n",
    "\n",
    "    # Convert boolean columns to int (0 or 1) for MySQL's TINYINT(1)\n",
    "    bool_cols = ['is_prime_member', 'is_festival_sale', 'is_prime_eligible']\n",
    "    for col in bool_cols:\n",
    "        if col in initial_df.columns: # Check if column exists\n",
    "            initial_df[col] = initial_df[col].astype(int)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Boolean column '{col}' not found in initial DataFrame.\")\n",
    "\n",
    "    # --- IMPORTANT: Ensure your SQL table schema matches the DataFrame columns ---\n",
    "    # The `transactions` table should be created with VARCHAR for IDs and DATE for dates.\n",
    "    # If you're using the CREATE TABLE statement I provided previously, it should match.\n",
    "\n",
    "    # Load into MySQL using chunks\n",
    "    # Using chunksize for large datasets to prevent memory issues and improve stability\n",
    "    chunksize = 10000 # Adjust chunksize as needed based on your system's memory\n",
    "\n",
    "    # Re-read CSV in chunks for the actual SQL insertion\n",
    "    # This ensures that if initial_df is huge, we don't hold it all in memory\n",
    "    # while also allowing pre-processing like boolean conversion per chunk.\n",
    "    for i, chunk in enumerate(pd.read_csv(transactions_csv_path, chunksize=chunksize, parse_dates=['order_date'])):\n",
    "        # Apply boolean conversion to each chunk\n",
    "        for col in bool_cols:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = chunk[col].astype(int)\n",
    "        \n",
    "        # Handle potential NaN values in numeric columns that might be NOT NULL in SQL\n",
    "        # For example, if 'delivery_days' or 'customer_rating' are NOT NULL in SQL\n",
    "        # and have NaNs in DataFrame, this will cause an error.\n",
    "        # Fill NaNs with a default value (e.g., 0, -1) or the mean/median if necessary.\n",
    "        # Example: chunk['delivery_days'].fillna(0, inplace=True)\n",
    "        # Example: chunk['customer_rating'].fillna(0.0, inplace=True)\n",
    "        \n",
    "        chunk.to_sql('transactions', con=engine, if_exists='append', index=False)\n",
    "        print(f\"  Loaded chunk {i+1} into 'transactions' table...\")\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded all data into 'transactions' table.\")\n",
    "\n",
    "    # Verify row count\n",
    "    with engine.connect() as conn:\n",
    "        trans_count = conn.execute(\"SELECT COUNT(*) FROM transactions\").fetchone()[0]\n",
    "        print(f\"üìä Final row count in 'transactions' table: {trans_count}\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"‚ùå Database error during loading: {e}\")\n",
    "    # Explicitly rollback the transaction if an error occurs\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()\n",
    "    print(\"‚ö†Ô∏è Rolled back transaction.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "\n",
    "    print(\"\\nüöÄ Transactions data loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c26048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Product catlog details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# ==========================\n",
    "# MySQL Connection Settings\n",
    "# ==========================\n",
    "USER = \"root\"              # Your MySQL username\n",
    "PASSWORD = \"test01!\"       # Your MySQL password\n",
    "HOST = \"localhost\"         # Host\n",
    "PORT = 3306                # MySQL port\n",
    "DB = \"amazon_india\"        # Database name\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# ==========================\n",
    "# Path to your product catalog CSV\n",
    "# ==========================\n",
    "products_csv_path = r\"D:\\amazon_india_project\\data\\Dataset\\amazon_india_products_catalog.csv\" # Adjust path as needed\n",
    "\n",
    "print(f\"üìÇ Attempting to load product catalog from '{products_csv_path}'...\")\n",
    "\n",
    "try:\n",
    "# Load product catalog dataset\n",
    "    products_df = pd.read_csv(products_csv_path)\n",
    "\n",
    "    # Convert boolean columns to int (0 or 1) for MySQL's TINYINT(1)\n",
    "    bool_cols = ['is_prime_eligible'] # Only this one seems to be boolean in your sample\n",
    "    for col in bool_cols:\n",
    "        if col in products_df.columns:\n",
    "            products_df[col] = products_df[col].astype(int)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Boolean column '{col}' not found in products DataFrame.\")\n",
    "\n",
    "    # --- IMPORTANT: Ensure your SQL table schema matches the DataFrame columns ---\n",
    "    # The `products` table should be created with VARCHAR for product_id.\n",
    "\n",
    "    # Load into MySQL\n",
    "    # Using chunksize for large datasets\n",
    "    chunksize = 500 # Products catalog is smaller, so chunksize can be adjusted\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(products_csv_path, chunksize=chunksize)):\n",
    "        # Apply boolean conversion to each chunk\n",
    "        for col in bool_cols:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = chunk[col].astype(int)\n",
    "        \n",
    "        chunk.to_sql('products', con=engine, if_exists='append', index=False)\n",
    "        print(f\"  Loaded chunk {i+1} into 'products' table...\")\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded all data into 'products' table.\")\n",
    "\n",
    "    # Verify row count\n",
    "    with engine.connect() as conn:\n",
    "        prod_count = conn.execute(\"SELECT COUNT(*) FROM products\").fetchone()[0]\n",
    "        print(f\"üìä Final row count in 'products' table: {prod_count}\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"‚ùå Database error during loading products: {e}\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()\n",
    "    print(\"‚ö†Ô∏è Rolled back transaction for products table.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred loading products: {e}\")\n",
    "\n",
    "    print(\"\\nüöÄ Product catalog loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique customer details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "USER = \"root\"\n",
    "PASSWORD = \"test01!\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "DB = \"amazon_india\"\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# Load transactions data (or just read the cleaned CSV again)\n",
    "transactions_df = pd.read_csv(r\"D:\\amazon_india_project\\data\\cleaned_amazon_india_complete_2015_2025.csv\")\n",
    "\n",
    "# Extract unique customer data\n",
    "customers_df = transactions_df[[\n",
    "    'customer_id', 'customer_city', 'customer_state',\n",
    "    'customer_tier', 'customer_spending_tier', 'customer_age_group'\n",
    "]].drop_duplicates(subset=['customer_id'])\n",
    "\n",
    "try:\n",
    "    customers_df.to_sql('customers', con=engine, if_exists='append', index=False)\n",
    "    print(f\"‚úÖ Loaded {len(customers_df)} unique customers into 'customers' table.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading customers table: {e}\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique dates for time dimension details to Mysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "USER = \"root\"\n",
    "PASSWORD = \"test01!\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 3306\n",
    "DB = \"amazon_india\"\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}\", pool_pre_ping=True)\n",
    "\n",
    "# Get all unique dates from transactions\n",
    "transactions_df = pd.read_csv(r\"D:\\amazon_india_project\\data\\cleaned_amazon_india_complete_2015_2025.csv\", parse_dates=['order_date'])\n",
    "unique_dates = pd.DataFrame({'date': transactions_df['order_date'].unique()})\n",
    "unique_dates['date'] = pd.to_datetime(unique_dates['date'])\n",
    "\n",
    "# Create time dimension attributes\n",
    "time_df = pd.DataFrame({\n",
    "    'date': unique_dates['date'],\n",
    "    'year': unique_dates['date'].dt.year,\n",
    "    'month': unique_dates['date'].dt.month,\n",
    "    'day': unique_dates['date'].dt.day,\n",
    "    'quarter': unique_dates['date'].dt.quarter,\n",
    "    'day_of_week': unique_dates['date'].dt.dayofweek,\n",
    "    'day_name': unique_dates['date'].dt.day_name(),\n",
    "    'month_name': unique_dates['date'].dt.month_name(),\n",
    "    'is_weekend': (unique_dates['date'].dt.dayofweek >= 5).astype(int) # 0=Mon, 6=Sun\n",
    "})\n",
    "\n",
    "try:\n",
    "    time_df.to_sql('time_dimension', con=engine, if_exists='append', index=False)\n",
    "    print(f\"‚úÖ Loaded {len(time_df)} rows into 'time_dimension' table.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading time_dimension table: {e}\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.rollback()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
